% Document class
\documentclass[12pt]{article}

% Packages
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry} 
\usepackage{a4wide, amsthm, amsmath, amssymb, graphicx, hyperref, bbm}
\usepackage{ragged2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}       % For writing code
\usepackage{minted}         % For colouring code
\usepackage{array}          % For making more flexible tables

% Optional packages
%\usepackage[utf8x]{inputenc}
%\usepackage{lipsum}

% New commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\soft}{\text{softmax}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% Set features 
% (include them within {} for them to only apply within a section of the document)
\lstset{language=Python}          % Set coding language (can change for each code-block)
\setlength{\parindent}{0pt}       % This is to avoid indentation of new paragraphs
\renewcommand{\arraystretch}{1.5} % Increase line spacing for all tables

% Environments
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
Reproducibility crisis: let’s build bridges, not walls
As a third year medical student interested in ophthalmology, I contacted ophthalmologist Dr Peter Koay of the University of Edinburgh, with whom we agreed to study the putative risk factors of pterygium, a disease of the eye. I located a few literary reviews on the topic, but I was not content with the absence of a systematic approach to organizing information of what effect size came from which study. I decided to systematically collect all studies on pterygium ever published and combine effect sizes reported in each paper weighted by sample size into a summary effect – I soon discovered that this was called a systematic review and that what I was trying to do was a fixed-effects meta-analysis (thankfully before submitting my conference abstract). Along the process, however, I discovered one last thing.
Even though I was able to synthesize all effect sizes into one pooled estimate of effect, I simply did not truly believe my results. The journey of gathering all of that evidence opened my eyes to a completely unexpected finding – the medical research I held as the Holy Grail and which to me was the untouchable ideal had barely given me any paper I felt I could trust. Upon further investigation, I learned about Prof. John Ioannidis and meta-research, the scientific study of how research is done and how we could possibly improve on it.
Using the principles of meta-research, we decided with Prof. Ioannidis to map how each paper had  studied pterygium and what specifications they had used in creating their estimates of effect (Serghiou et al., 2016). We created what we called data microarrays, which showed that no single paper, out of 63 eligible, either studied pterygium in the same way or had used the same specifications in estimating the effect of each risk factor on developing pterygium. As such, despite more than 80 years of research in the risk factors of pterygium, we could still not synthesize available data into a convincing or even coherent story of what may actually be a risk factor of pterygium. This amount of seeming “waste” in research to me was unacceptable (MacLeod et al., 2014), the most important problem in medicine and one directly relevant to what is now known as the “reproducibility crisis” or “replication crisis” (Baker, 2016).
The “Reproducibility Crisis”
Reproducibility has previously been defined by the National Science Foundation (NSF) as “the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator” (Bollen et al., 2015). It was then further categorized into “methods reproducibility” (to reproduce the methods of a paper), “results reproducibility” (to reproduce the results of a paper) and “inferential reproducibility” (to reproduce the inferences of a paper) (Goodman et al., 2016). 
Hints of a state of “crisis” were initially articulated in Ioannidis (2005), which illustrated that in the presence of a large set of improbable hypotheses and high opportunity for bias, most “statistically significant” findings should in fact be false positives. Even though this paper led to a series of arguments (Goodman and Greenland, 2007; Jager and Leek, 2014; series in Biostatistics Jan 2014 Vol 15 Issue 1), its findings have largely been corroborated by most collective attempts at replicating the literature.
Scientists at Bayer HealthCare in Germany could only validate 20-25% (13-17/67) of preclinical studies (Prinz et al., 2011) and at Amgen scientists could only replicate the findings of 6/53 landmark studies in cancer biology over 10 years despite strenuous efforts (Begley and Ellis, 2012). Similarly, in its abstract, the “Reproducibility Project: Psychology” (RPP) indicates that only 39/100 results tested were replicated (OSC, 2015) and the “Reproducibility Project: Cancer Biology” has so far replicated 3/10 studies (eLife Collection, 2018). Even though these attempts were qualitatively different and the implications of these attempts are not in fact as clear as it may seem, they do have in common that, despite fair attempts of replication, a majority of landmark findings could not be replicated in an even close to universally convincing manner.
Two other attempts are often cited as painting a picture less akin to a “crisis”: Camerer et al. (2016) could replicate 11/18 (61%) of laboratory-based papers in economics and Klein et al. (2014) could replicate 11/13. The former of these two tried to replicate the most “important” and statistically significant finding in each eligible paper; as such, its results are qualitatively very different from those of the aforementioned attempts, many of which include poorly-reproducible subgroup effects (i.e. effects only seen in a small subgroup of the population being studied). The latter only attempted to replicate papers studying a very specific effect – anchoring – and included a number of papers with effect sizes already known to be robust. Paramount caution is needed in interpreting the results of any replication attempt, but do the results so far available make sense?  
Studies in meta-research, have sought to answer exactly that (Begley, 2013). They have illustrated that most investigators still do not blind themselves (citations), do not use power and sample size calculations to identify an appropriate number of experiments (citations), often do not provide appropriate positive and negative controls (citations), use non-validated reagents (citations), use inappropriate statistical tests (citations), engage in data dredging (also known as p-hacking) and multiple testing to identify sufficiently small p-values (citations), engage in p-HARKing (hypothesizing after results are known) (citations), avoid publishing null results (citations)  and favor “surprising” results (citations), which are often more likely false. In light of these evidence, it is hardly surprising that most identified effects are not in fact true, or rather informative. However, is reproducibility what we should be really looking for? 
A table of effects contributing to “waste” in research
Indeed, reproducibility crisis or not, approaching research in terms of reproducibility may not in fact be the most constructive or efficient way to learn. The aim of science is to create associational knowledge and of experimental research to identify true effects on the basis of which we learn (citations). Unfortunately, we can never actually know the truth, but with accumulation of enough data, under the strong law of large numbers we can converge infinitesimally close to the truth. As such, the surrogate aim of experimental science is to accumulate evidence, on the basis of which we can estimate (1) magnitude and (2) direction of effect as accurately and precisely as required for further meaningful use. In other words, we should re-conceptualize scientific research as the attempt to gather data on the basis of which we can estimate an effect with a required amount of accuracy and precision and within which we no longer have successful or unsuccessful experiments, but rather more or less informative experiments.
A new framework
A table of all recommendations made in this essay
To do so, I hereby propose that we start thinking of scientific research as a unified continuum of probabilistic statements about hypotheses. Within this space, hypotheses accumulate waxing and waning favor, by synthesizing the results of each study to the results of each subsequent study. This is in contrast to the current system, where discrete original studies and replications operate in their own silos, leading to often fragmented, conflicting and uninterpretable results. One method for achieving that, is approaching science within the Bayesian framework of learning. 
Within the Bayesian framework, truth is conceptualized as a number of possible hypotheses. We do not know which of these hypotheses is true, but we can use current knowledge to construct such approximate probabilities of truth, known as priors. We then use further experimentation to accumulate data and on the basis of that data update these prior probabilities, now called posterior probabilities. As such, all research studies incorporate each other and come together within a unified framework; research is no longer seen as  right or wrong or true or false, but more or less informative. Attempts at replication, within this framework, are simply attempts to accumulate more evidence towards or away from a hypothesis.
As such, I propose that we abandon the “replication study” terminology and instead call subsequent studies either “supplementation” studies, “enrichment” studies or simply “follow-up” studies. The aim of such studies would be to (1) improve the accuracy of initial estimates, (2) improve precision of initial estimates and (3) confirm that the initial estimate was not confounded by previously hidden factors (such as, alternative confounding variables and bias). We can fulfil (1) and (2) by using machinery already available by Bayesian statistics and (3) by using classical frequentist statistics. The sample size of such studies would depend on how confident we want to be in our estimates of (1)-(2) when this study is complete and on the power we desire to identify a possibly hidden factor in (3). Such studies, would not only update our estimates of true effect, but also help identify previously unknown and potentially important factors, as in the case of the Cancer Reproducibility Project.
Figure explaining the process of research being proposed
In the Cancer Reproducibility Project, one of the teams was unable to reproduce the effect seen by the initial study (citation). Peer reviewers noticed that the tumors being used by the team attempting to replicate the effect were much smaller than the ones used by the initial study. When the researchers changed their experimental setup and used larger tumors, an effect very similar to the first was obtained (citation). Through this process, we learned that this effect is conditional on the size of the tumor and as such we generated knowledge that we could have otherwise skimmed over and which may have led to unreproducible results upon experimentation by pharmaceutical companies and eventually overlooking a potentially life-saving drug. 
The suggested framework helps conceptualize supplementation as an essential and natural aspect of science, rather than a threat of replication. It also helps in creating research that is immediately comparable to research previously done and in facilitating transition from a pilot study to a full study and from the full study to a supplementation study more naturally. Such methods can also help researchers report all of their experiments and all of their results, as all results attempt to estimate an effect size, rather than a p-value, which is then commonly used to brand an experiment as “successful” and publishable versus “failed” and unpublishable. However, it also suggests a different way of doing research, one that combines frequentist with Bayesian statistics and makes supplementation research as valuable as what may be called “original” research.
Operationalization
Within this concept of scientific research, the original “prototype” study authors can still run their analyses within the classical frequentist framework and spend most of their time optimizing their setup and protocols. However, the authors of the follow-up supplementation study can work on constructing a prior by using (1) time they saved by not having to create their own hypotheses and optimize experiments, (2) the results of the first study, (3) the discussion offered by the first study and (4) any other research they deem necessary. They can, in fact, construct multiple priors with different modelling choices: (1) a first “sceptic” prior can place little confidence in the results initially provided, a second (2) “agnostic” prior can place as much confidence as the current data command and (3) a third “optimistic” prior can place significant confidence in the initial results. The authors can then run their supplementation study and use their findings to update the prior probability of each hypothesis being true and improve our estimates of true effect – the aim is identification of true effect, not calculating a p-value. These studies do not have to be done in isolation and a follow-up supplementation study can incorporate a part purely interested in supplementing previous experiments and another part trying to study the hypotheses being tested using an alternative approach (Lawlor et al., 2016). This last kind of study can be thought of as using a “link” to the previous.
Figure explaining the process of supplementation
Instead of every study occurring in isolation, as happened in the aforementioned systematic review on pterygium, studies should be linked to each other and should aim to replicate aspects of a previous methodology within the population/sample that they are studying. This will not only update our previous estimates of effects, but will also make new findings within the new population more comparable to the previous population (improve transportability).
Current obstacles
There are three problems with operationalizing the above framework right away. First, at the moment, the methods of the initial paper are so uncertain that we cannot know whether an updated effect implies that previous research has not considered a potential confounder or that the experiments did not in fact accurately reproduce the initial experiments. We can solve this problem by offering more transparent methods and making all protocols available with the paper. Second, data is typically not readily available, which means that re-analyzing data using potentially preferred methods of analysis is difficult. We can solve this problem by making all deidentified data easily and openly available within already existing repositories (citation). Third, we can only falsify, not “truthify”, which means that we will only get progressively closer to the true effect by accumulating progressively more data.
In addition to these difficulties, we currently do not know how much supplementation research we need. Should we be providing supplemental data to every published study and every experiment within that study? Should we only be trying to supplement a random sample of studies? Should we be selecting for specific studies – high-impact studies will cause an “outbreak” of a lot more studies than less impactful studies – should we prioritize supplementing them? How often should we be running such supplementation efforts? The terrain of scientific research and different answers to these questions are likely to be apt for different fields and subfields of science – however, these questions need to be answered, theoretically and empirically.
Possibly table with answers to common counterarguments (e.g. how can we know that the first study was done well – we cannot, but we can let the data trump the prior; different fields have different needs – the signal to noise ratio in medicine is much higher than that of computer science)
Conclusion
Is there waste in research, yes there is. But, regardless of crisis or not, we can do a lot better in producing scientific knowledge and doing better truly matters. Transparent methods, open data and open file-drawers are key to learning more effectively and efficiently from the published literature. Supplementation studies on top of original “prototype” studies are an irreplaceable aspect of the scientific method and their combination with constructing a prior and further novel research can establish them as methods with equivalent scientific merit. Research should work on a continuum, research findings should be mindful of existing findings and new research should provide a direct link to previous research. Lastly, science is probably self-correcting, but we can probably “correct” faster. Let’s build bridges of communication from study to study and use those bridges to synthesize all available information into a coherent corpus of knowledge – walls around studies and practices benefit may benefit our egos, but not humanity. We still have a lot to learn about how we learn, but to learn we first need to accept that we do not actually know. 
To consider: Introduce the topic instead using the example of the 40 false positive results of ovarian cancer – replication does not necessarily imply true knowledge, only true effect (Rotmensch and Cole, 2000). A result and its conclusions can be highly reproducible, yet wrong because of hidden variables. Similarly, because of hidden variables, results may be non-reproducible, yet still be correct. This is why we need to make all of our data available, explicitly explain all of our protocols and transparently explain our thought process, so that others can know whether they are indeed replicating our methods and if they are not, what may be causing discrepancies in results.  
 
References
K. Bollen, J. T. Cacioppo, R. Kaplan, J. Krosnick, J. L. Olds, Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science (National Science Foundation, Arlington, VA, 2015).
eLife Collection. https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology 



% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}